{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06bd13f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import scrapy\n",
    "import os\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8dab34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify a folder to save the files\n",
    "DOWNLOAD_DIR = os.path.abspath(os.path.join(os.getcwd(),\"..\",\"ExampleDownloadSpot\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62e9ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the folder if it doesn't exist\n",
    "os.makedirs(DOWNLOAD_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3013a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileDownloaderSpider(scrapy.Spider):\n",
    "    name = \"file_downloader\"\n",
    "    start_urls = [\"https://gq.mines.gouv.qc.ca/documents/EXAMINE/\"] # main page\n",
    "    dry_run = True # toggle a no-download run of the code to test that the code is working\n",
    "    # getting information from links meeting the pattern of any 2 letters, any 2 digits, any 1 letter, at least 6 digits\n",
    "    \n",
    "\n",
    "    # go through main page links\n",
    "    def parse(self, response):\n",
    "        '''\n",
    "        Parses the main page and returns the full urls and related subfolder metadata that meet the criteria.\n",
    "\n",
    "        Args:\n",
    "            self (class): the instance of the FileDownloaderSpider.\n",
    "            response (scrapy.http.Response): the response from start_urls.\n",
    "\n",
    "        Returns:\n",
    "            The full urls and related subfolder metadata that meet the criteria.\n",
    "        '''\n",
    "        # 2 letters, any amount of characters, at least 6 digits in a row\n",
    "        pattern = r'^[a-zA-Z]{2}.*\\d{6,}'\n",
    "        links = response.css(\"pre a\")\n",
    "        for link in links:\n",
    "            link_text = link.css(\"::text\").get()\n",
    "            href = link.css('::attr(href)').get()\n",
    "\n",
    "            # only go through links with text meeting the specified pattern\n",
    "            # start by getting starting text that is desired\n",
    "            if link_text and link_text.startswith(\"DP\"):\n",
    "                \n",
    "                # only download files of links that match specified pattern\n",
    "                if re.match(pattern, link_text):\n",
    "\n",
    "                    full_url = urljoin(response.url, href)\n",
    "                    yield scrapy.Request(full_url, callback=self.parse_project_page,  meta={\"subfolder\": link_text})\n",
    "\n",
    "    # go through the results of main page \n",
    "    # this code only goes through results meeting the specified pattern because of the yield above\n",
    "    def parse_project_page(self, response):\n",
    "        '''\n",
    "        Using the metadata passed in parse(), Iterates over all non-header links in a given subfolder page and calls download_file with the given file URL and subfolder name passed in parse().\n",
    "\n",
    "        Args:\n",
    "            self (class): the instance of the FileDownloaderSpider.\n",
    "            response (scrapy.http.Response): the response of a given subfolder page that fits the criteria.\n",
    "\n",
    "        Returns:\n",
    "            Calls download_file on all non-header links with the given file URL and subfolder name passed in parse().\n",
    "        '''\n",
    "        header_strings = [\"Name\", \"Last modified\", \"Size\", \"Description\"]\n",
    "        subfolder = response.meta.get(\"subfolder\", \"misc\")\n",
    "\n",
    "        for link in response.css(\"pre a\"):\n",
    "            file_link_text = link.css(\"::text\").get()\n",
    "            href = link.css(\"::attr(href)\").get()\n",
    "\n",
    "            # exclude header strings\n",
    "            if file_link_text in header_strings:\n",
    "                continue\n",
    "\n",
    "            if href:\n",
    "                full_url = urljoin(response.url, href)\n",
    "                self.download_file(full_url, subfolder)\n",
    "\n",
    "    def download_file(self, file_url, subfolder):\n",
    "        '''\n",
    "        Creates a download folder named according to the relavant subfolder for a given link, which it them downloads.\n",
    "\n",
    "        Args:\n",
    "            self (class): the instance of the FileDownloaderSpider.\n",
    "            file_url (string): the full URL to a file that should be downloaded.\n",
    "            subfolder (string): comes from the link name on the main page. used to organize downloaded files.\n",
    "\n",
    "        Returns:\n",
    "            Downloaded files from the pages that meet the criteria, stored in the relevant subfolders.\n",
    "        '''\n",
    "\n",
    "        # dry_run = True\n",
    "        if self.dry_run:\n",
    "            self.logger.info(f\"This is a dry run. Would download: {file_url} into folder: {subfolder}\")\n",
    "\n",
    "        # dry_run = False\n",
    "        else:\n",
    "            # subfolder is based on the text that was clicked through on the main page\n",
    "            subfolder_path = os.path.join(DOWNLOAD_DIR, subfolder)\n",
    "            os.makedirs(subfolder_path, exist_ok=True)\n",
    "            local_filename = os.path.join(subfolder_path, os.path.basename(file_url))\n",
    "\n",
    "            try:\n",
    "                r = requests.get(file_url)\n",
    "                with open(local_filename, \"wb\") as f:\n",
    "                    f.write(r.content)\n",
    "                self.logger.info(f\"Downloaded: {file_url} -> {local_filename}\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Failed to download {file_url}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78180c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "process = CrawlerProcess(settings={\n",
    "    \"DOWNLOAD_DELAY\": 2\n",
    "    ,\"RANDOMIZE_DOWNLOAD_DELAY\": True\n",
    "    ,\"CONCURRENT_REQUESTS\": 1\n",
    "    ,\"AUTOTHROTTLE_ENABLED\": True # use autothrottle to dynamically change crawling speed based on server speeds\n",
    "    ,\"AUTOTHROTTLE_START_DELAY\": 1\n",
    "    ,\"AUTOTHROTTLE_MAX_DELAY\": 10\n",
    "    ,\"AUTOTHROTTLE_TARGET_CONCURRENCY\": 0.5\n",
    "    ,\"USER_AGENT\": \"Mozilla/5.0 (compatible; mybot/0.1)\"\n",
    "})\n",
    "process.crawl(FileDownloaderSpider)\n",
    "process.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
